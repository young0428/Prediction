{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 02:16:31.563890: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-23 02:16:31.582987: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-23 02:16:31.583010: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-23 02:16:31.583633: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-23 02:16:31.587510: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv1D, Conv2D, Dropout, ZeroPadding2D, SeparableConv2D, UpSampling2D, BatchNormalization, ReLU, LeakyReLU, Bidirectional\n",
    "from tensorflow.keras.layers import AveragePooling1D, Flatten, Conv1DTranspose, Conv2DTranspose, Reshape, Concatenate, AveragePooling2D, MaxPooling2D, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "import time\n",
    "from PreProcessing import SegmentsSSQCWT\n",
    "from tensorflow.keras import Sequential\t\n",
    "from collections import deque\n",
    "from readDataset import Segments2Data\n",
    "from CHB_processing.InfoProcessing import get_chb_interval_info\n",
    "from DQNModel import Build_DQN_model\n",
    "\n",
    "\n",
    "\n",
    "class n_step_memory:\n",
    "    def __init__(self, max_steps, gamma):\n",
    "        self.max_steps = max_steps\n",
    "        self.gamma = gamma\n",
    "        self.buffer = deque(maxlen=max_steps)\n",
    "        self.discounted_reward = 0.0\n",
    "    def add(self, experience):\n",
    "        # experience는 (state, action, reward, next_state, done)의 튜플\n",
    "        self.buffer.append(experience)\n",
    "        if len(self.buffer) >= self.max_steps:\n",
    "            return True\n",
    "        return False\n",
    "    def get(self):\n",
    "        if len(self.buffer) < self.max_steps:\n",
    "            return None\n",
    "        # N-step 보상을 계산.\n",
    "        reward = 0\n",
    "        for i in range(len(self.buffer)):\n",
    "            reward += self.buffer[i][2] * (self.gamma ** i)\n",
    "        state, action, _, _, _ = self.buffer[0]\n",
    "\n",
    "        # N-step 이후의 상태와 done 플래그를 가져옴\n",
    "        _, _, _, next_state, done = self.buffer[-1]\n",
    "        if done == True:\n",
    "            return None\n",
    "\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer.clear()\n",
    "        #self.discounted_reward = 0.0\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, beta=0.4, epsilon=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_increment = 0.001\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.min_prior = 0.0\n",
    "        self.max_prior = 1.0\n",
    "        \n",
    "        \n",
    "    def add(self, sample):\n",
    "        priority = self.max_prior\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(sample)\n",
    "            self.priorities.append(priority)\n",
    "        else:\n",
    "            self.buffer[self.position] = sample\n",
    "            self.priorities[self.position] = priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            raise ValueError(\"Not enough elements in the buffer to sample\")\n",
    "\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment])\n",
    "        total_priority = np.sum(self.priorities)\n",
    "        probabilities = self.priorities / total_priority\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        # Importance-sampling weights\n",
    "        weights = (len(self.buffer) * probabilities[indices]) ** (-self.beta)\n",
    "        weights /= np.max(weights)\n",
    "\n",
    "        return indices, samples, weights\n",
    "\n",
    "    def update_priorities(self, indices, errors):\n",
    "        for idx, error in zip(indices, errors):\n",
    "            priority = (np.abs(error) + self.epsilon) ** self.alpha\n",
    "            self.priorities[idx] = priority\n",
    "\n",
    "class EEGManager:\n",
    "    start_time = 0\n",
    "    cur_time = 0\n",
    "    cur_cwt = 0\n",
    "    def __init__(self, window_size, channels, data_type, game_duration = 30*60):\n",
    "        \n",
    "        self.game_duration = game_duration\n",
    "        self.cur_time = 0\n",
    "        self.edf_file_name = None\n",
    "        self.done = False\n",
    "        self.channels = channels\n",
    "        self.data_type = data_type\n",
    "        self.window_size = window_size\n",
    "        self.total_duration = 0\n",
    "        self.cur_game_duration = 0\n",
    "        self.step_size = 30\n",
    "        \n",
    "        chb_info = get_chb_interval_info()\n",
    "        self.total_time_flag = chb_info[0]\n",
    "        self.patient_enable_interval_info = chb_info[1]\n",
    "        self.patient_total_validation_duration = chb_info[2]\n",
    "        self.enable_period_flag = chb_info[3]\n",
    "        self.patient_names = list(self.total_time_flag.keys())\n",
    "        \n",
    "        \n",
    "        for name in self.patient_names:\n",
    "            self.total_duration += self.patient_total_validation_duration[name]\n",
    "        \n",
    "        self.patient_prob = [ self.patient_total_validation_duration[name] / self.total_duration for name in self.patient_names]\n",
    "\n",
    "    def select_patient(self):\n",
    "        self.selected_patient = np.random.choice(self.patient_names, 1, p = self.patient_prob)[0]\n",
    "        self.selected_patient_flag = self.total_time_flag[self.selected_patient]\n",
    "    def find_enable_adjacency_pos(self, intervals, cur_pos):\n",
    "        for i in range(len(intervals[1])-1):\n",
    "            if cur_pos > intervals[1][i][1] and cur_pos <= intervals[1][i+1][0] and intervals[1][i+1][1] - intervals[1][i+1][0] >= self.window_size:\n",
    "                cur_pos = intervals[1][i+1][0] + self.window_size\n",
    "                return cur_pos\n",
    "        # fail to find adjacency position\n",
    "        return -1\n",
    "    def select_initial_start_position(self, interval):\n",
    "        gap = self.window_size + self.game_duration\n",
    "        try_cnt = 0\n",
    "        while True:\n",
    "            \n",
    "            pos = random.randint(interval[0][0] + self.window_size, interval[0][1] - gap * 2)\n",
    "            if self.enable_period_flag[self.selected_patient][pos] == 0:\n",
    "                pos = self.find_enable_adjacency_pos(interval, pos)\n",
    "            if pos > interval[0][1] - gap or pos == -1:\n",
    "                try_cnt += 1\n",
    "                if try_cnt > 10:\n",
    "                    print(\"unable to select initial start position\")\n",
    "                    return -1\n",
    "                continue    \n",
    "            return pos\n",
    "    def find_file_by_pos(self, pos):\n",
    "        \n",
    "        for interval in self.choiced_interval[1]:\n",
    "            if pos >= interval[0] and pos <= interval[1] :\n",
    "                return interval\n",
    "        return -1\n",
    "    \n",
    "    def cal_ictal_distance(self, pos):\n",
    "        cnt = 0\n",
    "        distance = -1\n",
    "        while pos + cnt < len(self.selected_patient_flag):\n",
    "            if self.selected_patient_flag[pos+cnt] == 1:\n",
    "                distance = cnt\n",
    "                break\n",
    "            cnt += 1\n",
    "                \n",
    "        return distance\n",
    "    def init_game(self):\n",
    "        loop_cnt = 0\n",
    "        while True:\n",
    "            self.select_patient()\n",
    "            if self.patient_total_validation_duration[self.selected_patient] < self.game_duration:\n",
    "                continue\n",
    "            enable_interval = []\n",
    "            enabled_interval_total_length = 0 \n",
    "            for interval in self.patient_enable_interval_info[self.selected_patient]:\n",
    "                interval_length = interval[0][1] - interval[0][0]\n",
    "                if interval_length > (self.game_duration + self.window_size)*2:\n",
    "                    enable_interval.append(interval)\n",
    "                    enabled_interval_total_length += interval_length\n",
    "                    \n",
    "            # escape the loop when fail to select interval 5 times\n",
    "            if len(enable_interval) == 0:\n",
    "                loop_cnt += 1\n",
    "                if loop_cnt >= 5:\n",
    "                    print(\"unable to select interval!\")\n",
    "                    break\n",
    "                continue\n",
    "            \n",
    "            interval_prob = [ (interval[0][1] - interval[0][0])/enabled_interval_total_length for interval in enable_interval]\n",
    "            self.choiced_interval = enable_interval[np.random.choice(list(range(len(enable_interval))), 1, p = interval_prob)[0]]\n",
    "            start_time = self.select_initial_start_position(self.choiced_interval)\n",
    "            if start_time == -1 :\n",
    "                loop_cnt += 1\n",
    "                if loop_cnt >= 5:\n",
    "                    print(\"unable to select interval!\")\n",
    "                    break\n",
    "                continue\n",
    "            \n",
    "            self.cur_pos = start_time\n",
    "            self.cur_cwt = self.pos2cwt(self.cur_pos)\n",
    "            if self.cur_cwt == -1:\n",
    "                print(\"fail to read data from edf file\")\n",
    "                continue\n",
    "            self.alarm_state = 0\n",
    "            self.cur_game_duration = 0\n",
    "            self.done = False\n",
    "            loop_cnt = 0\n",
    "            break\n",
    "            \n",
    "        return self.cur_pos, (self.cur_cwt, self.alarm_state)\n",
    "    def pos2cwt(self, pos):\n",
    "        interval = self.find_file_by_pos(pos)\n",
    "\n",
    "        file_name = interval[2]\n",
    "        pos_in_file = pos - interval[0]\n",
    "        self.segment = [[file_name, pos_in_file-self.window_size, self.window_size]]\n",
    "     \n",
    "        eeg_data = Segments2Data(self.segment, self.data_type, self.channels)\n",
    "        if type(eeg_data) == int:\n",
    "            print(\"fail to read data from edf file\")\n",
    "            return -1\n",
    "        # [1, channel_num, self.window_size * sampling_rate]       \n",
    "        eeg_data.shape = (1, eeg_data.shape[2])\n",
    "\n",
    "            \n",
    "        \n",
    "        # [1, self.window_size * sampling_rate]\n",
    "        \n",
    "        state = SegmentsSSQCWT(eeg_data, sampling_rate=200, scale_resolution=128)\n",
    "        return state\n",
    "    \n",
    "    def cal_reward(self, cur_pos, alarm_state, action):\n",
    "        reward = 0\n",
    "        if action == 1:\n",
    "            reward -= 1\n",
    "        if (3 in self.selected_patient_flag[cur_pos - self.window_size : cur_pos]) or (3 in self.selected_patient_flag[cur_pos - self.window_size : cur_pos]):\n",
    "            if action == 1 and alarm_state == 0:\n",
    "                reward = 10\n",
    "        return reward\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = self.cal_reward(self.cur_pos, self.alarm_state, action)\n",
    "        if action == 1:\n",
    "            self.alarm_state = 1\n",
    "            \n",
    "        self.cur_game_duration += self.step_size\n",
    "        next_pos = self.cur_pos + self.step_size\n",
    "        if 0 in self.enable_period_flag[self.selected_patient][next_pos-self.window_size:next_pos]:\n",
    "            next_pos = self.find_enable_adjacency_pos(self.choiced_interval, next_pos)\n",
    "        # terminated ?            \n",
    "        if next_pos == -1 or self.cur_game_duration >= self.game_duration:\n",
    "            done = True\n",
    "            next_pos = -1\n",
    "            next_state = None\n",
    "            next_cwt = None\n",
    "        else:    \n",
    "            next_cwt = self.pos2cwt(next_pos)\n",
    "            if next_cwt == -1:\n",
    "                print(\"fail to load edf file!\")\n",
    "                return -1,-1,-1,-1\n",
    "            next_state = (next_cwt, self.alarm_state)\n",
    "            \n",
    "        self.cur_pos = next_pos\n",
    "        self.cur_cwt = next_cwt\n",
    "\n",
    "        return next_pos, next_state, reward, done\n",
    "        \n",
    "\n",
    "class RainbowDQNAgent:\n",
    "    # INITIALIZING THE Q-PARAMETERS\n",
    "    max_episodes = 200  # Set total number of episodes to train agent on.\n",
    "    batch_size = 16\n",
    "    action_num = 2\n",
    "    Vmin = -10\n",
    "    Vmax = 10\n",
    "    num_atoms = 51\n",
    "    \n",
    "    # Exploration parameters\n",
    "    epsilon = 1.0                 # Exploration rate\n",
    "    max_epsilon = 1.0             # Exploration probability at start\n",
    "    min_epsilon = 0.01            # Minimum exploration probability \n",
    "    decay_rate = 0.005            # Exponential decay rate for exploration prob\n",
    "    gamma = 0.99\n",
    "\n",
    "    scores = []\n",
    "    def __init__(self,  window_size, channels, data_type, step_size = 30):\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        \n",
    "        self.TAU = 0.1\n",
    "        self.lr = 0.001\n",
    "        self.soft_update = False\n",
    "        self.z = np.linspace(self.Vmin, self.Vmax, self.num_atoms)\n",
    "        \n",
    "        self.memory = PrioritizedReplayBuffer(capacity=20000)\n",
    "        self.n_step = 5\n",
    "        self.n_step_memory = n_step_memory(self.n_step, self.gamma)\n",
    "        \n",
    "        cwt_inputs = tf.keras.layers.Input(shape=(128,6000, 1))\n",
    "        alarm_state_inputs = tf.keras.layers.Input(shape=(1))\n",
    "        \n",
    "        self.train_start = 1000\n",
    "        \n",
    "        self.optimizers = tf.keras.optimizers.Adam(learning_rate=self.lr, )\n",
    "        self.criterion = tf.keras.losses.CategoricalCrossentropy()\n",
    "        \n",
    "        self.dqn = Build_DQN_model(cwt_inputs, alarm_state_inputs, num_atom=self.num_atoms)\n",
    "        self.dqn_target = Build_DQN_model(cwt_inputs, alarm_state_inputs, num_atom=self.num_atoms)\n",
    "        self.target_hard_update()\n",
    "\n",
    "        self.game_manager = EEGManager(window_size, channels, data_type)\n",
    "        \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        cwt_image, alarm = state\n",
    "        cwt_image = np.expand_dims(cwt_image, axis=-1)\n",
    "        state = (cwt_image, alarm)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action = np.random.choice()\n",
    "        else:\n",
    "            action = self.get_optimal_aciton(state)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def get_optimal_aciton(self, state):\n",
    "        z = self.dqn.predict_on_batch(state)\n",
    "        q = tf.reduce_sum(tf.multiply(z, self.z),axis=-1)\n",
    "        return np.argmax(q)\n",
    "    \n",
    "    def init_game(self):\n",
    "        cur_pos, cur_state = self.game_manager.init_game()\n",
    "        self.n_step_memory.reset()\n",
    "        return cur_pos, cur_state\n",
    "\n",
    "    def target_hard_update(self):\n",
    "        if not self.soft_update:\n",
    "            weights = self.dqn.get_weights()\n",
    "            self.dqn_target.set_weights(weights)\n",
    "            return\n",
    "        if self.soft_update:\n",
    "            q_model_theta = self.dqn.model.get_weights()\n",
    "            dqn_target_theta = self.dqn_target.model.get_weights()\n",
    "            counter = 0\n",
    "            for q_weight, target_weight in zip(q_model_theta, dqn_target_theta):\n",
    "                target_weight = target_weight * (1-self.TAU) + q_weight * self.TAU\n",
    "                dqn_target_theta[counter] = target_weight\n",
    "                counter += 1\n",
    "            self.dqn_target.set_weights(dqn_target_theta)\n",
    "            \n",
    "    def step(self, action):\n",
    "        \n",
    "        cur_pos = self.game_manager.cur_pos\n",
    "        cur_state = (self.game_manager.cur_cwt, self.game_manager.cur_alarm_state)\n",
    "        next_pos, next_state, reward, done = self.game_manager.step(action)\n",
    "        \n",
    "        \n",
    "        \n",
    "        n_step_memory_full = self.n_step_memory.add((cur_state, action, reward, next_state, done))\n",
    "        \n",
    "        # if n_step_memory is filled\n",
    "        if n_step_memory_full:\n",
    "            sample = self.n_step_memory.get()\n",
    "            self.memory.add(sample)\n",
    "        \n",
    "        return next_pos, next_state, reward, done\n",
    "\n",
    "    def update_model(self):\n",
    "        indices, samples, is_weights = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        elementalwisw_loss, loss = self.compute_dqn_loss(samples, is_weights)\n",
    "        self.memory.update_priorities(indices, elementalwisw_loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def compute_dqn_loss(self, samples, is_weights):\n",
    "        with tf.device(self.device):\n",
    "            states, actions, rewards, next_states, dones = samples\n",
    "            \n",
    "            delta_z = (self.Vmax - self.Vmin) / (self.atom_size - 1)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Get current action probabilities\n",
    "                dist = self.dqn.predict_on_batch(states)\n",
    "                log_p = tf.math.log(dist[tf.range(self.batch_size), actions])\n",
    "\n",
    "                # Double DQN\n",
    "                with tf.name_scope(\"target_q_values\"):\n",
    "                    next_action = tf.argmax(self.dqn.predict_on_batch(next_states), axis=1)\n",
    "                    next_dist = tf.stop_gradient(self.dqn_target.predict_on_batch(next_states))\n",
    "                    next_dist = next_dist[tf.range(self.batch_size), next_action]\n",
    "\n",
    "                    # Project distribution onto support\n",
    "                    t_z = rewards + (1 - dones) * self.gamma * self.support\n",
    "                    t_z = tf.clip_by_value(t_z, self.Vmin, self.Vmax)\n",
    "                    b = (t_z - self.Vmin) / delta_z\n",
    "                    l = tf.floor(b)\n",
    "                    u = tf.ceil(b)\n",
    "\n",
    "                    offset = tf.range(self.batch_size) * self.atom_size\n",
    "                    offset = tf.expand_dims(offset, axis=1)\n",
    "\n",
    "                    proj_dist = tf.zeros_like(next_dist)\n",
    "                    proj_dist += tf.tensor_scatter_nd_add(proj_dist, tf.cast(l + offset, tf.int32), (next_dist * (u - b)))\n",
    "                    proj_dist += tf.tensor_scatter_nd_add(proj_dist, tf.cast(u + offset, tf.int32), (next_dist * (b - l)))\n",
    "\n",
    "                # Calculate element-wise loss\n",
    "                elementwise_loss = -tf.reduce_sum(proj_dist * log_p, axis=1)\n",
    "                loss = tf.reduce_mean(elementwise_loss * is_weights)\n",
    "\n",
    "        # Calculate gradients and apply optimizer updates\n",
    "        gradients = tape.gradient(loss, self.dqn.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.dqn.trainable_variables))\n",
    "\n",
    "        return elementwise_loss, loss\n",
    "\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 02:17:23.555129: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 02:17:23.555475: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 02:17:23.555501: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 02:17:23.558355: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 02:17:23.558404: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 02:17:23.558417: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 02:17:23.683117: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 02:17:23.683204: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 02:17:23.683215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-02-23 02:17:23.683252: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 02:17:23.683279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21472 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "agent = RainbowDQNAgent(window_size=30, channels = ['FP1-F7'], data_type = 'chb_one_ch', step_size=30)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_pos, cur_state = agent.init_game()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
