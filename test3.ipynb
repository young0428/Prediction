{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'readDataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/hy105/Desktop/Prediction/test3.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/hy105/Desktop/Prediction/test3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequence\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/hy105/Desktop/Prediction/test3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/hy105/Desktop/Prediction/test3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mreadDataset\u001b[39;00m \u001b[39mimport\u001b[39;00m LoadDataset, Interval2Segments, Segments2Data\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/hy105/Desktop/Prediction/test3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mAutoEncoder\u001b[39;00m \u001b[39mimport\u001b[39;00m FullChannelEncoder, FullChannelDecoder, FullChannelEncoder_test, FullChannelDecoder_test\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/hy105/Desktop/Prediction/test3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mLSTMmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m LSTMLayer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'readDataset'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('~'))\n",
    "if module_path not in sys.path:\n",
    "\tsys.path.append(module_path+\"/mnt/c/Users/hy105/Desktop/Prediction\")\n",
    "\n",
    "\n",
    "os.environ['TF_GPU_THREAD_MODE']='gpu_private'\n",
    "os.environ['TF_GPU_THREAD_COUNT']='32'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import operator\n",
    "import matplotlib as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, Conv2D, Dropout, ZeroPadding2D, SeparableConv2D, UpSampling2D\n",
    "from tensorflow.keras.layers import AveragePooling1D, Flatten, Conv1DTranspose, Conv2DTranspose, Reshape, Concatenate, AveragePooling2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "\n",
    "from readDataset import LoadDataset, Interval2Segments, Segments2Data\n",
    "from AutoEncoder import FullChannelEncoder, FullChannelDecoder, FullChannelEncoder_test, FullChannelDecoder_test\n",
    "from LSTMmodel import LSTMLayer\n",
    "from sklearn.model_selection import KFold\n",
    "from PreProcessing import GetBatchIndexes\n",
    "\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "# %%\n",
    "class autoencoder_generator(Sequence):\n",
    "    def __init__(self,type_1_data, type_2_data, type_3_data, batch_size):\n",
    "        \n",
    "        self.type_1_data = type_1_data\n",
    "        self.type_2_data = type_2_data\n",
    "        self.type_3_data = type_3_data\n",
    "\n",
    "        self.type_1_data_len = len(type_1_data)\n",
    "        self.type_2_data_len = len(type_2_data)\n",
    "        \n",
    "        type_3_sampled_for_balance = type_3_data[np.random.choice(len(type_3_data), int((self.type_1_data_len + self.type_2_data_len)*1.5),replace=False)]\n",
    "        self.type_3_data_len = len(type_3_sampled_for_balance)\n",
    "\n",
    "        self.batch_num = int((self.type_1_data_len + self.type_2_data_len + self.type_3_data_len)/batch_size)\n",
    "\n",
    "        self.type_1_batch_indexes = GetBatchIndexes(self.type_1_data_len, self.batch_num)\n",
    "        self.type_2_batch_indexes = GetBatchIndexes(self.type_2_data_len, self.batch_num)\n",
    "        self.type_3_batch_indexes = GetBatchIndexes(self.type_3_data_len, self.batch_num)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_num\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seg = np.concatenate((self.type_1_data[self.type_1_batch_indexes[idx]], self.type_2_data[self.type_2_batch_indexes[idx]], self.type_3_data[self.type_3_batch_indexes[idx]]))\n",
    "        X_batch = Segments2Data(input_seg)\n",
    "        return X_batch, X_batch\n",
    "\n",
    "# %%\n",
    "if __name__=='__main__':\n",
    "    window_size = 2\n",
    "    overlap_sliding_size = 1\n",
    "    normal_sliding_size = window_size\n",
    "    state = ['preictal_ontime', 'ictal', 'preictal_late', 'preictal_early', 'postictal','interictal']\n",
    "\n",
    "    train_info_file_path = \"D:/SNU_DATA/patient_info_train.csv\"\n",
    "    test_info_file_path = \"D:/SNU_DATA/patient_info_test.csv\"\n",
    "    edf_file_path = \"D:/SNU_DATA\"\n",
    "\n",
    "\n",
    "    train_interval_set = LoadDataset(train_info_file_path)\n",
    "    train_segments_set = {}\n",
    "\n",
    "    test_interval_set = LoadDataset(test_info_file_path)\n",
    "    test_segments_set = {}\n",
    "\n",
    "    # 상대적으로 데이터 갯수가 적은 것들은 window_size 2초에 sliding_size 1초로 overlap 시켜 데이터 증강\n",
    "    for state in ['preictal_ontime', 'ictal', 'preictal_late', 'preictal_early']:\n",
    "        train_segments_set[state] = Interval2Segments(train_interval_set[state],edf_file_path, window_size, overlap_sliding_size)\n",
    "        test_segments_set[state] = Interval2Segments(test_interval_set[state],edf_file_path, window_size, overlap_sliding_size)\n",
    "        \n",
    "\n",
    "    for state in ['postictal', 'interictal']:\n",
    "        train_segments_set[state] = Interval2Segments(train_interval_set[state],edf_file_path, window_size, normal_sliding_size)\n",
    "        test_segments_set[state] = Interval2Segments(test_interval_set[state],edf_file_path, window_size, normal_sliding_size)\n",
    "\n",
    "    # type 1은 True Label데이터 preictal_ontime\n",
    "    # type 2는 특별히 갯수 맞춰줘야 하는 데이터\n",
    "    # type 3는 나머지\n",
    "\n",
    "    # AutoEncoder 단계에서는 1:1:3\n",
    "\n",
    "    train_type_1 = np.array(train_segments_set['preictal_ontime'])\n",
    "    train_type_2 = np.array(train_segments_set['ictal'] + train_segments_set['preictal_early'] + train_segments_set['preictal_late'])\n",
    "    train_type_3 = np.array(train_segments_set['postictal'] + train_segments_set['interictal'])\n",
    "\n",
    "    test_type_1 = np.array(test_segments_set['preictal_ontime'])\n",
    "    test_type_2 = np.array(test_segments_set['ictal'] + test_segments_set['preictal_early'] + test_segments_set['preictal_late'])\n",
    "    test_type_3 = np.array(test_segments_set['postictal'] + test_segments_set['interictal'])\n",
    "\n",
    "    fold_n = 5\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    epochs = 1\n",
    "    batch_size = 500   # 한번의 gradient update시마다 들어가는 데이터의 사이즈\n",
    "    total_len = len(train_type_1)+len(train_type_2)\n",
    "    total_len = int(total_len*2.5) # 데이터 비율 2:2:6\n",
    "\n",
    "    type_1_kfold_set = kf.split(train_type_1)\n",
    "    type_2_kfold_set = kf.split(train_type_2)\n",
    "    type_3_kfold_set = kf.split(train_type_3)\n",
    "\n",
    "\n",
    "    for _ in range(fold_n):\n",
    "        encoder_inputs = Input(shape=(21,512,1))\n",
    "        encoder_outputs = FullChannelEncoder(encoded_feature_num=64,inputs = encoder_inputs)\n",
    "        decoder_outputs = FullChannelDecoder(encoder_outputs)\n",
    "        autoencoder_model = Model(inputs=encoder_inputs, outputs=decoder_outputs)\n",
    "        autoencoder_model.compile(optimizer = 'Adam', loss='mse',)\n",
    "\n",
    "        (type_1_train_indexes, type_1_val_indexes) = next(type_1_kfold_set)\n",
    "        (type_2_train_indexes, type_2_val_indexes) = next(type_2_kfold_set)\n",
    "        (type_3_train_indexes, type_3_val_indexes) = next(type_3_kfold_set)\n",
    "\n",
    "        type_1_data_len = len(type_1_train_indexes)\n",
    "        type_2_data_len = len(type_2_train_indexes)\n",
    "        type_3_data_len = int((type_1_data_len + type_2_data_len)*1.5)\n",
    "        train_batch_num = int((type_1_data_len + type_2_data_len + type_3_data_len)/batch_size)\n",
    "\n",
    "        type_1_data_len = len(type_1_val_indexes)\n",
    "        type_2_data_len = len(type_2_val_indexes)\n",
    "        type_3_data_len = int((type_1_data_len + type_2_data_len)*1.5)\n",
    "        val_batch_num = int((type_1_data_len + type_2_data_len + type_3_data_len)/batch_size)\n",
    "        logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "        tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
    "                                                        histogram_freq = 1,\n",
    "                                                        profile_batch = '500,520')\n",
    "        train_generator = autoencoder_generator(train_type_1[type_1_train_indexes], train_type_2[type_2_train_indexes], train_type_3[type_3_train_indexes],batch_size)\n",
    "        validation_generator = autoencoder_generator(train_type_1[type_1_val_indexes], train_type_2[type_2_val_indexes], train_type_3[type_3_val_indexes],batch_size)\n",
    "# %%\n",
    "        history = autoencoder_model.fit_generator(\n",
    "                    train_generator,\n",
    "                    epochs = epochs,\n",
    "                    steps_per_epoch =  train_batch_num,\n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps = val_batch_num,\n",
    "                    workers=8,\n",
    "                    use_multiprocessing=True,\n",
    "                    callbacks= [ tboard_callback ]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/~\n"
     ]
    }
   ],
   "source": [
    "module_path = \"/mnt/c/Users/hy105/Desktop/Prediction\"\n",
    "if module_path not in sys.path:\n",
    "\tsys.path.append(module_path)\n",
    "\t\n",
    "print(module_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
