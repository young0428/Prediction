{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv1D, Conv2D, Dropout, ZeroPadding2D, SeparableConv2D, UpSampling2D\n",
    "from tensorflow.keras.layers import AveragePooling1D, Flatten, Conv1DTranspose, Conv2DTranspose, Reshape, Concatenate, AveragePooling2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "\n",
    "from readDataset import LoadDataset, Interval2Segments, Segments2Data\n",
    "from AutoEncoder import FullChannelEncoder, FullChannelDecoder, FullChannelEncoder_test, FullChannelDecoder_test\n",
    "from LSTMmodel import LSTMLayer\n",
    "from sklearn.model_selection import KFold\n",
    "from PreProcessing import GetBatchIndexes\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import operator\n",
    "import matplotlib as plt\n",
    "import os\n",
    "\n",
    "os.environ['TF_GPU_THREAD_MODE']='gpu_private'\n",
    "os.environ['TF_GPU_THREAD_COUNT']='16'\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder_generator(Sequence):\n",
    "    def __init__(self,type_1_data, type_2_data, type_3_data, batch_size):\n",
    "        \n",
    "        self.type_1_data = type_1_data\n",
    "        self.type_2_data = type_2_data\n",
    "        self.type_3_data = type_3_data\n",
    "\n",
    "        self.type_1_data_len = len(type_1_data)\n",
    "        self.type_2_data_len = len(type_2_data)\n",
    "        \n",
    "        type_3_sampled_for_balance = type_3_data[np.random.choice(len(type_3_data), int((self.type_1_data_len + self.type_2_data_len)*1.5),replace=False)]\n",
    "        self.type_3_data_len = len(type_3_sampled_for_balance)\n",
    "\n",
    "        self.batch_num = int((self.type_1_data_len + self.type_2_data_len + self.type_3_data_len)/batch_size)\n",
    "\n",
    "        self.type_1_batch_indexes = GetBatchIndexes(self.type_1_data_len, self.batch_num)\n",
    "        self.type_2_batch_indexes = GetBatchIndexes(self.type_2_data_len, self.batch_num)\n",
    "        self.type_3_batch_indexes = GetBatchIndexes(self.type_3_data_len, self.batch_num)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_num\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seg = np.concatenate((self.type_1_data[self.type_1_batch_indexes[idx]], self.type_2_data[self.type_2_batch_indexes[idx]], self.type_3_data[self.type_3_batch_indexes[idx]]))\n",
    "        X_batch = Segments2Data(input_seg)\n",
    "        return X_batch, X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    window_size = 2\n",
    "    overlap_sliding_size = 1\n",
    "    normal_sliding_size = window_size\n",
    "    state = ['preictal_ontime', 'ictal', 'preictal_late', 'preictal_early', 'postictal','interictal']\n",
    "\n",
    "    train_info_file_path = \"D:/SNU_DATA/patient_info_train.csv\"\n",
    "    test_info_file_path = \"D:/SNU_DATA/patient_info_test.csv\"\n",
    "    edf_file_path = \"D:/SNU_DATA\"\n",
    "\n",
    "\n",
    "    train_interval_set = LoadDataset(train_info_file_path)\n",
    "    train_segments_set = {}\n",
    "\n",
    "    test_interval_set = LoadDataset(test_info_file_path)\n",
    "    test_segments_set = {}\n",
    "\n",
    "    # 상대적으로 데이터 갯수가 적은 것들은 window_size 2초에 sliding_size 1초로 overlap 시켜 데이터 증강\n",
    "    for state in ['preictal_ontime', 'ictal', 'preictal_late', 'preictal_early']:\n",
    "        train_segments_set[state] = Interval2Segments(train_interval_set[state],edf_file_path, window_size, overlap_sliding_size)\n",
    "        test_segments_set[state] = Interval2Segments(test_interval_set[state],edf_file_path, window_size, overlap_sliding_size)\n",
    "        \n",
    "\n",
    "    for state in ['postictal', 'interictal']:\n",
    "        train_segments_set[state] = Interval2Segments(train_interval_set[state],edf_file_path, window_size, normal_sliding_size)\n",
    "        test_segments_set[state] = Interval2Segments(test_interval_set[state],edf_file_path, window_size, normal_sliding_size)\n",
    "\n",
    "    # type 1은 True Label데이터 preictal_ontime\n",
    "    # type 2는 특별히 갯수 맞춰줘야 하는 데이터\n",
    "    # type 3는 나머지\n",
    "\n",
    "    # AutoEncoder 단계에서는 1:1:3\n",
    "\n",
    "    train_type_1 = np.array(train_segments_set['preictal_ontime'])\n",
    "    train_type_2 = np.array(train_segments_set['ictal'] + train_segments_set['preictal_early'] + train_segments_set['preictal_late'])\n",
    "    train_type_3 = np.array(train_segments_set['postictal'] + train_segments_set['interictal'])\n",
    "\n",
    "    test_type_1 = np.array(test_segments_set['preictal_ontime'])\n",
    "    test_type_2 = np.array(test_segments_set['ictal'] + test_segments_set['preictal_early'] + test_segments_set['preictal_late'])\n",
    "    test_type_3 = np.array(test_segments_set['postictal'] + test_segments_set['interictal'])\n",
    "\n",
    "    fold_n = 5\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    epochs = 100\n",
    "    batch_size = 500   # 한번의 gradient update시마다 들어가는 데이터의 사이즈\n",
    "    total_len = len(train_type_1)+len(train_type_2)\n",
    "    total_len = int(total_len*2.5) # 데이터 비율 2:2:6\n",
    "\n",
    "    type_1_kfold_set = kf.split(train_type_1)\n",
    "    type_2_kfold_set = kf.split(train_type_2)\n",
    "    type_3_kfold_set = kf.split(train_type_3)\n",
    "\n",
    "\n",
    "    for _ in range(fold_n):\n",
    "        encoder_inputs = Input(shape=(21,512,1))\n",
    "        encoder_outputs = FullChannelEncoder_test(encoded_feature_num=64,inputs = encoder_inputs)\n",
    "        decoder_outputs = FullChannelDecoder_test(encoder_outputs)\n",
    "        autoencoder_model = Model(inputs=encoder_inputs, outputs=decoder_outputs)\n",
    "        autoencoder_model.compile(optimizer = 'Adam', loss='mse',)\n",
    "\n",
    "        (type_1_train_indexes, type_1_val_indexes) = next(type_1_kfold_set)\n",
    "        (type_2_train_indexes, type_2_val_indexes) = next(type_2_kfold_set)\n",
    "        (type_3_train_indexes, type_3_val_indexes) = next(type_3_kfold_set)\n",
    "\n",
    "        type_1_data_len = len(type_1_train_indexes)\n",
    "        type_2_data_len = len(type_2_train_indexes)\n",
    "        type_3_data_len = int((type_1_data_len + type_2_data_len)*1.5)\n",
    "        train_batch_num = int((type_1_data_len + type_2_data_len + type_3_data_len)/batch_size)\n",
    "\n",
    "        type_1_data_len = len(type_1_val_indexes)\n",
    "        type_2_data_len = len(type_2_val_indexes)\n",
    "        type_3_data_len = int((type_1_data_len + type_2_data_len)*1.5)\n",
    "        val_batch_num = int((type_1_data_len + type_2_data_len + type_3_data_len)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder_model.fit_generator(\n",
    "            autoencoder_generator(train_type_1[type_1_train_indexes], train_type_2[type_2_train_indexes], train_type_3[type_3_train_indexes],batch_size),\n",
    "            epochs = epochs,\n",
    "            steps_per_epoch =  train_batch_num,\n",
    "            validation_data = autoencoder_generator(train_type_1[type_1_val_indexes], train_type_2[type_2_val_indexes], train_type_3[type_3_val_indexes],batch_size),\n",
    "            validation_steps = val_batch_num,\n",
    "            workers=16,\n",
    "            use_multiprocessing=True\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
