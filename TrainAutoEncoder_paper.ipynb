{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_GPU_THREAD_MODE']='gpu_private'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import operator\n",
    "import matplotlib as plt\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "from readDataset import LoadDataset, Interval2Segments, Segments2Data\n",
    "import AutoEncoder\n",
    "from LSTMmodel import LSTMLayer\n",
    "from sklearn.model_selection import KFold\n",
    "import PreProcessing\n",
    "from TestAutoEncoder import test_ae\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder_generator(Sequence):\n",
    "    def __init__(self,type_1_data, type_2_data, type_3_data, batch_size, model_name, gen_type):\n",
    "    \n",
    "        self.ratio_type_1 = [2,2,2,2]\n",
    "        self.ratio_type_2 = [2,2,2,2]\n",
    "        self.ratio_type_3 = [6,6,6,6]\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.check = True\n",
    "        self.epoch = 0\n",
    "        self.cnt = 0\n",
    "        self.update_period = 5*2\n",
    "        self.type_1_data = type_1_data\n",
    "        self.type_2_data = type_2_data\n",
    "        self.type_3_data = type_3_data\n",
    "        self.test_on = False\n",
    "        self.ratio_idx = 0\n",
    "        self.model_name = model_name\n",
    "        self.gen_type = gen_type\n",
    "\n",
    "        self.update_data()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.epoch += 1\n",
    "        self.update_data()\n",
    "        if self.gen_type == \"train\":\n",
    "            if self.epoch % 6 == 0:\n",
    "                try:\n",
    "                    test_ae(int(self.epoch/2), 5,2,128, self.model_name)\n",
    "                except:\n",
    "                    print(\"Fail to generate test fig\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_num\n",
    "\n",
    "    def update_data(self):\n",
    "        # 데이터 밸런스를 위해 데이터 밸런스 조절 및 resampling\n",
    "        if self.epoch/self.update_period < 4:\n",
    "            self.ratio_idx = int(self.epoch/self.update_period)\n",
    "        else:\n",
    "            self.ratio_idx = 3\n",
    "        # ratio에 따라 데이터 갯수 정함\n",
    "        self.type_1_sampled_len = len(self.type_1_data)\n",
    "        self.type_2_sampled_len = min(int((self.type_1_sampled_len/self.ratio_type_1[self.ratio_idx])*self.ratio_type_2[self.ratio_idx]),len(self.type_2_data))\n",
    "        self.type_3_sampled_len = int((self.type_1_sampled_len/self.ratio_type_1[self.ratio_idx])*self.ratio_type_3[self.ratio_idx])\n",
    "        # Sampling mask 생성\n",
    "        self.type_2_sampling_mask = sorted(np.random.choice(len(self.type_2_data), self.type_2_sampled_len, replace=False))\n",
    "        self.type_3_sampling_mask = sorted(np.random.choice(len(self.type_3_data), self.type_3_sampled_len, replace=False))\n",
    "\n",
    "        self.type_2_sampled = self.type_2_data[self.type_2_sampling_mask]\n",
    "        self.type_3_sampled = self.type_3_data[self.type_3_sampling_mask]\n",
    "\n",
    "        self.batch_num = int((self.type_1_sampled_len + self.type_2_sampled_len + self.type_3_sampled_len)/self.batch_size)\n",
    "        \n",
    "        self.type_1_batch_indexes = PreProcessing.GetBatchIndexes(self.type_1_sampled_len, self.batch_num)\n",
    "        self.type_2_batch_indexes = PreProcessing.GetBatchIndexes(self.type_2_sampled_len, self.batch_num)\n",
    "        self.type_3_batch_indexes = PreProcessing.GetBatchIndexes(self.type_3_sampled_len, self.batch_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seg = np.concatenate((self.type_1_data[self.type_1_batch_indexes[idx]], \n",
    "                                    self.type_2_sampled[self.type_2_batch_indexes[idx]], \n",
    "                                    self.type_3_sampled[self.type_3_batch_indexes[idx]]))\n",
    "        \n",
    "        x_batch = Segments2Data(input_seg)\n",
    "        #x_batch = PreProcessing.AbsFFT(x_batch)\n",
    "        #y_batch = PreProcessing.FilteringSegments(x_batch)\n",
    "\n",
    "        if (idx+1) % int(self.batch_num / 3) == 0 and self.gen_type == \"train\":\n",
    "            self.type_3_sampling_mask = sorted(np.random.choice(len(self.type_3_data), self.type_3_sampled_len, replace=False))\n",
    "            self.type_3_sampled = self.type_3_data[self.type_3_sampling_mask]\n",
    "            self.type_3_batch_indexes = PreProcessing.GetBatchIndexes(self.type_3_sampled_len, self.batch_num)\n",
    "  \n",
    "        return x_batch, x_batch\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name):\n",
    "    window_size = 5\n",
    "    overlap_sliding_size = 2\n",
    "    normal_sliding_size = window_size\n",
    "    sr = 128\n",
    "    check = [True]\n",
    "    state = ['preictal_ontime', 'ictal', 'preictal_late', 'preictal_early', 'postictal','interictal']\n",
    "\n",
    "    # for WSL\n",
    "    train_info_file_path = \"/host/d/SNU_DATA/SNU_patient_info_train.csv\"\n",
    "    test_info_file_path = \"/host/d/SNU_DATA/SNU_patient_info_test.csv\"\n",
    "    edf_file_path = \"/host/d/SNU_DATA\"\n",
    "\n",
    "    ## for window\n",
    "    # train_info_file_path = \"D:/SNU_DATA/patient_info_train.csv\"\n",
    "    # test_info_file_path = \"D:/SNU_DATA/patient_info_test.csv\"\n",
    "    # edf_file_path = \"D:/SNU_DATA\"\n",
    "\n",
    "\n",
    "    train_interval_set = LoadDataset(train_info_file_path)\n",
    "    train_segments_set = {}\n",
    "\n",
    "    test_interval_set = LoadDataset(test_info_file_path)\n",
    "    test_segments_set = {}\n",
    "\n",
    "    # 상대적으로 데이터 갯수가 적은 것들은 window_size 2초에 sliding_size 1초로 overlap 시켜 데이터 증강\n",
    "    for state in ['preictal_ontime', 'ictal', 'preictal_late', 'preictal_early']:\n",
    "        train_segments_set[state] = Interval2Segments(train_interval_set[state],edf_file_path, window_size, overlap_sliding_size)\n",
    "        \n",
    "    for state in ['postictal', 'interictal']:\n",
    "        train_segments_set[state] = Interval2Segments(train_interval_set[state],edf_file_path, window_size, normal_sliding_size)\n",
    "\n",
    "\n",
    "    # type 1은 True Label데이터 preictal_ontime\n",
    "    # type 2는 특별히 갯수 맞춰줘야 하는 데이터\n",
    "    # type 3는 나머지\n",
    "\n",
    "    # AutoEncoder 단계에서는 1:1:3\n",
    "\n",
    "    train_type_1 = np.array(train_segments_set['preictal_ontime'])\n",
    "    train_type_2 = np.array(train_segments_set['ictal'] + train_segments_set['preictal_early'] + train_segments_set['preictal_late'])\n",
    "    train_type_3 = np.array(train_segments_set['postictal'] + train_segments_set['interictal'])\n",
    "\n",
    "    fold_n = 5\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    epochs = 100\n",
    "    batch_size = 500   # 한번의 gradient update시마다 들어가는 데이터의 사이즈\n",
    "    total_len = len(train_type_1)+len(train_type_2)\n",
    "    total_len = int(total_len*2.5) # 데이터 비율 2:2:6\n",
    "\n",
    "    type_1_kfold_set = kf.split(train_type_1)\n",
    "    type_2_kfold_set = kf.split(train_type_2)\n",
    "    type_3_kfold_set = kf.split(train_type_3)\n",
    "\n",
    "\n",
    "\n",
    "    (type_1_train_indexes, type_1_val_indexes) = next(type_1_kfold_set)\n",
    "    (type_2_train_indexes, type_2_val_indexes) = next(type_2_kfold_set)\n",
    "    (type_3_train_indexes, type_3_val_indexes) = next(type_3_kfold_set)\n",
    "    checkpoint_path = f\"AutoEncoder/{model_name}/cp.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    encoder_inputs = Input(shape=(21,sr*window_size,1))\n",
    "    encoder_outputs = AutoEncoder.FullChannelEncoder_paper_base(inputs = encoder_inputs)\n",
    "    decoder_outputs = AutoEncoder.FullChannelDecoder_paper_base(encoder_outputs)\n",
    "    autoencoder_model = Model(inputs=encoder_inputs, outputs=decoder_outputs)\n",
    "    autoencoder_model.compile(optimizer = 'RMSprop', loss='mse')\n",
    "    if os.path.exists(f\"./AutoEncoder/{model_name}\"):\n",
    "        print(\"Model Loaded!\")\n",
    "        autoencoder_model = tf.keras.models.load_model(checkpoint_path)\n",
    "\n",
    "    autoencoder_model.summary()\n",
    "\n",
    "    logs = \"logs/\" + model_name + \"-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "    tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
    "                                                    histogram_freq = 1,\n",
    "                                                    profile_batch = '1,400')\n",
    "    \n",
    "\n",
    "    # Create a callback that saves the model's weights\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                    save_best_only=True,\n",
    "                                                    verbose=1)\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                            verbose=1,\n",
    "                                                            patience=10,\n",
    "                                                            restore_best_weights=True)\n",
    "    \n",
    "    train_generator = autoencoder_generator(train_type_1[type_1_train_indexes], \n",
    "                                            train_type_2[type_2_train_indexes],\n",
    "                                            train_type_3[type_3_train_indexes],\n",
    "                                            batch_size,\n",
    "                                            model_name,\n",
    "                                            \"train\"\n",
    "                                            \n",
    "                                            )\n",
    "    validation_generator = autoencoder_generator(train_type_1[type_1_val_indexes], \n",
    "                                                    train_type_2[type_2_val_indexes],\n",
    "                                                    train_type_3[type_3_val_indexes],\n",
    "                                                    batch_size,\n",
    "                                                    model_name,\n",
    "                                                    \"val\"\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    history = autoencoder_model.fit_generator(\n",
    "                train_generator,\n",
    "                epochs = epochs,\n",
    "                validation_data = validation_generator,\n",
    "                use_multiprocessing=True,\n",
    "                workers=12,\n",
    "                shuffle=False,\n",
    "                callbacks= [ tboard_callback, cp_callback, early_stopping ]\n",
    "                )\n",
    "    \n",
    "    with open(f'./AutoEncoder/{model_name}/trainHistoryDict', 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "\n",
    ""
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}